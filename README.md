# Text Classification using LSTM, Word2Vec, and Adam Optimizer

This project presents a text classification model built with Long Short-Term Memory (LSTM) networks and Word2Vec embeddings, optimized using the Adam optimizer. The dataset includes textual data labeled into various categories. By employing pre-trained Word2Vec embeddings, the textual data is converted into vectorized representations. These vectors are passed through an LSTM architecture, known for capturing sequential patterns in text. The Adam optimizer ensures efficient and adaptive training of the model, enabling faster convergence.


**Key Features:**

- Data Preprocessing: Text cleaning, tokenization, and vectorization using Word2Vec.
- LSTM Model: Sequential LSTM layers for effective sequence learning and text classification.
- Word2Vec Embeddings: Pre-trained word vectors for meaningful text representation.
- Adam Optimizer: Adaptive optimization technique to enhance model performance.
- Performance Evaluation: Accuracy and loss metrics are used to evaluate the model's performance.
